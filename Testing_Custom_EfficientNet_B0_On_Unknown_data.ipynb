{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuW5cy6XJLie"
   },
   "source": [
    "Dataset : https://drive.google.com/drive/folders/1Eb8VgRaabdD00AdFDnAOi3xmG4uzHxdO?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmaizrZ0Y5_1"
   },
   "source": [
    "### Testing and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124025,
     "status": "ok",
     "timestamp": 1728530168135,
     "user": {
      "displayName": "samruddhi kale",
      "userId": "01288157686112405710"
     },
     "user_tz": -330
    },
    "id": "6pUY3WnGaUd5",
    "outputId": "4df1e2ec-4f9c-4a0e-a49d-e2af502386eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-deefc85bf76f>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA_E_2602463.png: 1\n",
      "LA_E_8974701.png: 1\n",
      "LA_E_5224411.png: 1\n",
      "LA_E_9629099.png: 1\n",
      "LA_E_1572528.png: 1\n",
      "LA_E_4505498.png: 1\n",
      "LA_E_8478765.png: 1\n",
      "LA_E_3651348.png: 1\n",
      "LA_E_1147384.png: 1\n",
      "LA_E_8023690.png: 1\n",
      "LA_E_5835667.png: 1\n",
      "LA_E_5762961.png: 1\n",
      "LA_E_5978185.png: 1\n",
      "LA_E_6479378.png: 1\n",
      "LA_E_8579243.png: 1\n",
      "LA_E_6210128.png: 0\n",
      "LA_E_4924002.png: 0\n",
      "LA_E_4793366.png: 1\n",
      "LA_E_1411561.png: 0\n",
      "LA_E_4455219.png: 1\n",
      "LA_E_8996199.png: 0\n",
      "LA_E_2001230.png: 1\n",
      "LA_E_1671939.png: 1\n",
      "LA_E_4271320.png: 1\n",
      "LA_E_1890993.png: 1\n",
      "LA_E_5009251.png: 0\n",
      "LA_E_1830307.png: 1\n",
      "LA_E_5624940.png: 1\n",
      "LA_E_4306145.png: 0\n",
      "LA_E_2407430.png: 1\n",
      "LA_E_2218533.png: 1\n",
      "LA_E_6927350.png: 1\n",
      "LA_E_6273262.png: 1\n",
      "LA_E_7850546.png: 1\n",
      "LA_E_6782590.png: 1\n",
      "LA_E_3358578.png: 1\n",
      "LA_E_8535405.png: 1\n",
      "LA_E_3203713.png: 1\n",
      "LA_E_8445983.png: 1\n",
      "LA_E_5742134.png: 1\n",
      "LA_E_7172071.png: 1\n",
      "LA_E_2733438.png: 0\n",
      "LA_E_9928530.png: 1\n",
      "LA_E_6394190.png: 1\n",
      "LA_E_9055968.png: 1\n",
      "LA_E_5367430.png: 1\n",
      "LA_E_4116166.png: 1\n",
      "LA_E_4042297.png: 1\n",
      "LA_E_9872320.png: 1\n",
      "LA_E_1468379.png: 1\n",
      "LA_E_3252530.png: 1\n",
      "LA_E_6902026.png: 1\n",
      "LA_E_2137584.png: 1\n",
      "LA_E_8179051.png: 1\n",
      "LA_E_9183774.png: 1\n",
      "LA_E_6028023.png: 1\n",
      "LA_E_3379384.png: 1\n",
      "LA_E_4584303.png: 1\n",
      "LA_E_6634561.png: 1\n",
      "LA_E_7218987.png: 1\n",
      "LA_E_3508317.png: 1\n",
      "LA_E_4419177.png: 1\n",
      "LA_E_7051084.png: 1\n",
      "LA_E_2798340.png: 1\n",
      "LA_E_6388833.png: 1\n",
      "LA_E_1206529.png: 1\n",
      "LA_E_9741315.png: 0\n",
      "LA_E_3115325.png: 1\n",
      "LA_E_8889291.png: 1\n",
      "LA_E_3191091.png: 1\n",
      "LA_E_9985043.png: 0\n",
      "LA_E_9260961.png: 1\n",
      "LA_E_6735159.png: 1\n",
      "LA_E_9088851.png: 1\n",
      "LA_E_1498385.png: 1\n",
      "LA_E_A9903225.png: 1\n",
      "LA_E_3673066.png: 1\n",
      "LA_E_3470067.png: 1\n",
      "LA_E_1772316.png: 1\n",
      "LA_E_7194628.png: 1\n",
      "LA_E_4390995.png: 1\n",
      "LA_E_3652225.png: 1\n",
      "LA_E_1522694.png: 1\n",
      "LA_E_4211799.png: 1\n",
      "LA_E_4403921.png: 1\n",
      "LA_E_8810051.png: 1\n",
      "LA_E_8480584.png: 1\n",
      "LA_E_2890258.png: 1\n",
      "LA_E_3457833.png: 1\n",
      "LA_E_4386143.png: 1\n",
      "LA_E_9507978.png: 1\n",
      "LA_E_4837497.png: 1\n",
      "LA_E_2055590.png: 1\n",
      "LA_E_1719710.png: 0\n",
      "LA_E_9285585.png: 1\n",
      "LA_E_8405786.png: 1\n",
      "LA_E_2499236.png: 1\n",
      "LA_E_6824070.png: 1\n",
      "LA_E_5679901.png: 1\n",
      "LA_E_8938067.png: 1\n",
      "LA_E_1952631.png: 1\n",
      "LA_E_5947243.png: 1\n",
      "LA_E_9605375.png: 1\n",
      "LA_E_7267349.png: 0\n",
      "LA_E_9701827.png: 1\n",
      "LA_E_5620926.png: 1\n",
      "LA_E_3008073.png: 1\n",
      "LA_E_6006096.png: 1\n",
      "LA_E_3324859.png: 1\n",
      "LA_E_9513615.png: 1\n",
      "LA_E_3476133.png: 1\n",
      "LA_E_6121357.png: 1\n",
      "LA_E_7662852.png: 1\n",
      "LA_E_7839857.png: 0\n",
      "LA_E_6490164.png: 1\n",
      "LA_E_A5815457.png: 1\n",
      "LA_E_3738137.png: 1\n",
      "LA_E_6212486.png: 1\n",
      "LA_E_9826136.png: 1\n",
      "LA_E_3858652.png: 1\n",
      "LA_E_9301809.png: 0\n",
      "LA_E_1773182.png: 1\n",
      "LA_E_2893165.png: 1\n",
      "LA_E_9017208.png: 1\n",
      "LA_E_9832903.png: 1\n",
      "LA_E_8826682.png: 1\n",
      "LA_E_6690047.png: 1\n",
      "LA_E_6893969.png: 1\n",
      "LA_E_4444286.png: 1\n",
      "LA_E_1949642.png: 1\n",
      "LA_E_7244977.png: 1\n",
      "LA_E_6091845.png: 1\n",
      "LA_E_7585260.png: 1\n",
      "LA_E_6005570.png: 1\n",
      "LA_E_1137091.png: 1\n",
      "LA_E_9489395.png: 1\n",
      "LA_E_4102704.png: 0\n",
      "LA_E_5641746.png: 1\n",
      "LA_E_5225469.png: 1\n",
      "LA_E_2427374.png: 1\n",
      "LA_E_4773943.png: 1\n",
      "LA_E_1917583.png: 1\n",
      "LA_E_4594103.png: 1\n",
      "LA_E_A8390905.png: 1\n",
      "LA_E_3133961.png: 1\n",
      "LA_E_2427668.png: 1\n",
      "LA_E_2238657.png: 0\n",
      "LA_E_4147485.png: 1\n",
      "LA_E_5906512.png: 0\n",
      "LA_E_2936126.png: 1\n",
      "LA_E_1954156.png: 1\n",
      "LA_E_8533431.png: 1\n",
      "LA_E_9339706.png: 1\n",
      "LA_E_6703258.png: 1\n",
      "LA_E_1469287.png: 1\n",
      "LA_E_3057526.png: 1\n",
      "LA_E_6035442.png: 1\n",
      "LA_E_4432840.png: 1\n",
      "LA_E_9379873.png: 1\n",
      "LA_E_7061027.png: 1\n",
      "LA_E_9545521.png: 0\n",
      "LA_E_2186004.png: 1\n",
      "LA_E_4343208.png: 1\n",
      "LA_E_6739466.png: 1\n",
      "LA_E_A9210248.png: 1\n",
      "LA_E_9731896.png: 1\n",
      "LA_E_9752426.png: 1\n",
      "LA_E_9707372.png: 1\n",
      "LA_E_1204517.png: 1\n",
      "LA_E_5823782.png: 1\n",
      "LA_E_4960473.png: 1\n",
      "LA_E_6973514.png: 0\n",
      "LA_E_4918197.png: 1\n",
      "LA_E_5963102.png: 1\n",
      "LA_E_5248764.png: 1\n",
      "LA_E_9486227.png: 1\n",
      "LA_E_1409473.png: 1\n",
      "LA_E_5387338.png: 1\n",
      "LA_E_7323190.png: 1\n",
      "LA_E_9094036.png: 1\n",
      "LA_E_3925062.png: 1\n",
      "LA_E_2789674.png: 0\n",
      "LA_E_8169975.png: 1\n",
      "LA_E_7745420.png: 1\n",
      "LA_E_7935605.png: 1\n",
      "LA_E_4686464.png: 1\n",
      "LA_E_5600401.png: 1\n",
      "LA_E_3336719.png: 0\n",
      "LA_E_1220492.png: 1\n",
      "LA_E_4320656.png: 1\n",
      "LA_E_1234428.png: 1\n",
      "LA_E_4909157.png: 1\n",
      "LA_E_5574532.png: 1\n",
      "LA_E_1709002.png: 1\n",
      "LA_E_3145431.png: 1\n",
      "LA_E_6555289.png: 1\n",
      "LA_E_1291334.png: 1\n",
      "LA_E_9642897.png: 1\n",
      "LA_E_4092168.png: 1\n",
      "LA_E_4610471.png: 1\n",
      "LA_E_2724234.png: 1\n",
      "LA_E_1679492.png: 1\n",
      "LA_E_8463157.png: 1\n",
      "LA_E_1020598.png: 1\n",
      "LA_E_A4032623.png: 1\n",
      "LA_E_6568244.png: 1\n",
      "LA_E_3425195.png: 1\n",
      "LA_E_4701926.png: 1\n",
      "LA_E_7503879.png: 1\n",
      "LA_E_6844059.png: 1\n",
      "LA_E_A6716830.png: 1\n",
      "LA_E_9005008.png: 1\n",
      "LA_E_7059509.png: 1\n",
      "LA_E_9808705.png: 1\n",
      "LA_E_4778912.png: 1\n",
      "LA_E_1039058.png: 1\n",
      "LA_E_6060705.png: 1\n",
      "LA_E_6063874.png: 1\n",
      "LA_E_9485854.png: 1\n",
      "LA_E_8043133.png: 1\n",
      "LA_E_6882547.png: 1\n",
      "LA_E_7830839.png: 1\n",
      "LA_E_9161326.png: 1\n",
      "LA_E_3903852.png: 1\n",
      "LA_E_8048885.png: 1\n",
      "LA_E_9059753.png: 0\n",
      "LA_E_7997032.png: 1\n",
      "LA_E_9038405.png: 1\n",
      "LA_E_5660177.png: 1\n",
      "LA_E_7092645.png: 1\n",
      "LA_E_1443485.png: 1\n",
      "LA_E_2192799.png: 1\n",
      "LA_E_8172142.png: 1\n",
      "LA_E_A6924100.png: 1\n",
      "LA_E_5535879.png: 0\n",
      "LA_E_7275417.png: 1\n",
      "LA_E_7184741.png: 1\n",
      "LA_E_7875716.png: 1\n",
      "LA_E_4403980.png: 1\n",
      "LA_E_4221253.png: 1\n",
      "LA_E_1239732.png: 1\n",
      "LA_E_9239403.png: 1\n",
      "LA_E_2855242.png: 1\n",
      "LA_E_9479558.png: 1\n",
      "LA_E_5685470.png: 1\n",
      "LA_E_5977884.png: 1\n",
      "LA_E_2425774.png: 1\n",
      "LA_E_7626732.png: 1\n",
      "LA_E_7109274.png: 0\n",
      "LA_E_9968824.png: 1\n",
      "LA_E_7699238.png: 0\n",
      "LA_E_4404123.png: 1\n",
      "LA_E_8668479.png: 1\n",
      "LA_E_1006831.png: 1\n",
      "LA_E_9411881.png: 0\n",
      "LA_E_9750172.png: 1\n",
      "LA_E_3047393.png: 1\n",
      "LA_E_6327699.png: 0\n",
      "LA_E_8802306.png: 1\n",
      "LA_E_5609818.png: 1\n",
      "LA_E_3725579.png: 1\n",
      "LA_E_5699797.png: 1\n",
      "LA_E_1710015.png: 1\n",
      "LA_E_8758360.png: 1\n",
      "LA_E_2873865.png: 1\n",
      "LA_E_6368203.png: 1\n",
      "LA_E_3953842.png: 1\n",
      "LA_E_7883686.png: 1\n",
      "LA_E_4249461.png: 1\n",
      "LA_E_1999542.png: 1\n",
      "LA_E_8952347.png: 1\n",
      "LA_E_3548794.png: 1\n",
      "LA_E_1755197.png: 1\n",
      "LA_E_8478947.png: 1\n",
      "LA_E_5378312.png: 1\n",
      "LA_E_1357041.png: 1\n",
      "LA_E_8998743.png: 0\n",
      "LA_E_9344985.png: 1\n",
      "LA_E_3998141.png: 1\n",
      "LA_E_9222715.png: 0\n",
      "LA_E_9684721.png: 1\n",
      "LA_E_2707901.png: 1\n",
      "LA_E_3592263.png: 1\n",
      "LA_E_5150603.png: 1\n",
      "LA_E_3799019.png: 1\n",
      "LA_E_5213123.png: 1\n",
      "LA_E_3552665.png: 1\n",
      "LA_E_6068314.png: 1\n",
      "LA_E_5432446.png: 1\n",
      "LA_E_3306343.png: 1\n",
      "LA_E_8177126.png: 1\n",
      "LA_E_3853883.png: 1\n",
      "LA_E_2383069.png: 1\n",
      "LA_E_7144542.png: 1\n",
      "LA_E_2890067.png: 1\n",
      "LA_E_7055547.png: 1\n",
      "LA_E_8335879.png: 1\n",
      "LA_E_6540584.png: 1\n",
      "LA_E_2255317.png: 1\n",
      "LA_E_2196801.png: 1\n",
      "Predictions have been successfully generated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the model class\n",
    "class RES_EfficientCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RES_EfficientCNN, self).__init__()\n",
    "        # Load a pre-trained EfficientNet model\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)  # Use the latest weights\n",
    "        # Replace the classifier with a new one for your specific task\n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = RES_EfficientCNN(num_classes=2)\n",
    "\n",
    "# Load the trained model\n",
    "model_path = '/content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/Custom_EfficientNet_B0.pth'  # Adjust the path to Custom_EfficientNet_B0 model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the image transformation (adjust as per your training process)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize for 3 channels\n",
    "])\n",
    "\n",
    "# Source directory where the generated spectrogram images are stored\n",
    "test_spectrogram_dir = '/content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/selected_audio_files_testing_spectrograms'\n",
    "\n",
    "# Process each spectrogram image and make predictions\n",
    "predictions = []\n",
    "for file_name in os.listdir(test_spectrogram_dir):\n",
    "    if file_name.endswith('.png'):  # Ensure we're working with image files\n",
    "        image_path = os.path.join(test_spectrogram_dir, file_name)\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "        image = image.convert('RGB')  # Convert to RGB to match model input\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            output = model(image)\n",
    "            _, predicted = torch.max(output, 1)  # Get the class with the highest score\n",
    "\n",
    "        predictions.append((file_name, predicted.item()))  # Store filename and prediction\n",
    "\n",
    "# Optionally, save or print the predictions\n",
    "for filename, prediction in predictions:\n",
    "    print(f\"{filename}: {prediction}\")\n",
    "\n",
    "# If you want to save the predictions to a file\n",
    "output_file = '/content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/predictions.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for filename, prediction in predictions:\n",
    "        f.write(f\"{filename}: {prediction}\\n\")\n",
    "\n",
    "print(\"Predictions have been successfully generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adIl84WuJYue"
   },
   "source": [
    "## Convert txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1728563747126,
     "user": {
      "displayName": "samruddhi kale",
      "userId": "01288157686112405710"
     },
     "user_tz": -330
    },
    "id": "AGXkSSNBZLP-",
    "outputId": "cd64b67a-1407-43a1-c5cf-b9e57675d098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels saved to /content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6e3939b9b596>:8: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  predicted_labels_df = pd.read_csv(predicted_labels_txt, delim_whitespace=True, header=None, names=['FileName', 'SpoofType'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the predicted labels txt file\n",
    "predicted_labels_txt = '/content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/predictions.txt'\n",
    "\n",
    "# Load the predicted labels into a DataFrame with column names matching the original labels file\n",
    "predicted_labels_df = pd.read_csv(predicted_labels_txt, delim_whitespace=True, header=None, names=['FileName', 'SpoofType'])\n",
    "\n",
    "# Remove file extensions from 'FileName' column\n",
    "predicted_labels_df['FileName'] = predicted_labels_df['FileName'].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "# Path to save the predicted labels CSV file\n",
    "predicted_labels_csv = '/content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/predictions.csv'\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "predicted_labels_df.to_csv(predicted_labels_csv, index=False)\n",
    "\n",
    "print(f\"Predicted labels saved to {predicted_labels_csv}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1728564038749,
     "user": {
      "displayName": "samruddhi kale",
      "userId": "01288157686112405710"
     },
     "user_tz": -330
    },
    "id": "if1mJ8wxcXe0",
    "outputId": "3b6e7112-5068-4ff7-d9b9-24385b81d114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 80.55%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the original and predicted label files\n",
    "original_labels_csv = '/content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/selected_audio_files_testing_labels.csv'\n",
    "predicted_labels_csv = '/content/drive/MyDrive/ADD_ASV_DATA/LA/ASVspoof2019_LA_train/ADD_CNN/Testing_RES_On_Unknown_Data/predictions.csv'\n",
    "\n",
    "# Read the original and predicted CSV files into DataFrames\n",
    "original_df = pd.read_csv(original_labels_csv)\n",
    "predicted_df = pd.read_csv(predicted_labels_csv)\n",
    "\n",
    "# Map the numeric predictions to their string equivalents\n",
    "predicted_df['SpoofType'] = predicted_df['SpoofType'].map({0: 'bonafide', 1: 'spoof'})\n",
    "\n",
    "# Merge the two DataFrames on the 'FileName' column to align the labels\n",
    "merged_df = pd.merge(original_df, predicted_df, on='FileName', suffixes=('_original', '_predicted'))\n",
    "\n",
    "# Calculate accuracy by comparing the 'SpoofType' columns\n",
    "accuracy = accuracy_score(merged_df['SpoofType_original'], merged_df['SpoofType_predicted'])\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Testing Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hZ8otoQcX6o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
